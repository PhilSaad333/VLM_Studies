{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolVLM2 GUI Lab Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook mirrors the local repo structure for Colab usage. It loads AGUVIS datasets, previews samples, and runs zero-shot checks with `smolagents/SmolVLM2-2.2B-Instruct`.\n\nSteps:\n1. Mount Drive and clone the repo.\n2. Install requirements.\n3. Sample Stage-1/Stage-2/ScreenSpot examples.\n4. Run the base model on selected prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\ndrive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nREPO_DIR = '/content/VLM_Studies'\nif not os.path.exists(REPO_DIR):\n    !git clone https://github.com/PhilSaad333/VLM_Studies.git {REPO_DIR}\nos.chdir(REPO_DIR)\nprint('Working dir:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path.cwd()))\n\nfrom data_sources.aguvis.stage1 import load_stage1, STAGE1_CONFIGS\nfrom data_sources.aguvis.stage2 import load_stage2, STAGE2_CONFIGS\nfrom data_sources.screenspot import load_screenspot\n\nprint('Stage-1 configs:', STAGE1_CONFIGS)\nprint('Stage-2 configs:', STAGE2_CONFIGS[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-1 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\nfrom IPython.display import display\n\nconfig = 'webui350k'\nsample = next(iter(load_stage1(config, streaming=True)))\nprint('Config:', config)\nprint('User:', sample['user'])\nprint('Assistant:', sample['assistant'])\ndisplay(sample['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-2 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'mind2web-l2'\nsample2 = next(iter(load_stage2(config, streaming=True)))\nprint('Config:', config)\nprint('System:', sample2['system'][:200])\nprint('User:', sample2['user'])\nprint('Assistant:', sample2['assistant'][:400])\ndisplay(sample2['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScreenSpot-v2 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenspot_sample = next(iter(load_screenspot(split='validation', streaming=True)))\nprint('Instruction:', screenspot_sample.instruction)\nprint('Source:', screenspot_sample.source)\ndisplay(screenspot_sample.draw_bbox())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SmolVLM2-2.2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\nimport torch\n\nMODEL_NAME = 'smolagents/SmolVLM2-2.2B-Instruct'\nprocessor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\nmodel = AutoModelForVision2Seq.from_pretrained(MODEL_NAME, trust_remote_code=True, device_map='auto')\nmodel.eval()\nprint('Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Trial (Stage-1 instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n    {\n        'role': 'user',\n        'content': [\n            {'type': 'image', 'image': sample['image']},\n            {'type': 'text', 'text': sample['user']}\n        ]\n    }\n]\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors='pt').to(model.device)\nwith torch.no_grad():\n    generated = model.generate(**inputs, max_new_tokens=64)\noutput = processor.tokenizer.decode(generated[0], skip_special_tokens=True)\nprint(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n- Extend with Stage-2 multi-step prompts.\n- Log outputs to Drive for qualitative comparison.\n- Integrate bounding-box tool once available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}