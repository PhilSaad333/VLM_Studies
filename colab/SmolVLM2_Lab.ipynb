{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolVLM2 GUI Lab Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook mirrors the local repo structure for Colab usage. It loads AGUVIS datasets, previews samples, and runs zero-shot checks with `smolagents/SmolVLM2-2.2B-Instruct`.\n",
    "\n",
    "Steps:\n",
    "1. Mount Drive and clone the repo.\n",
    "2. Install requirements.\n",
    "3. Sample Stage-1/Stage-2/ScreenSpot examples.\n",
    "4. Run the base model on selected prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REPO_DIR = '/content/VLM_Studies'\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/PhilSaad333/VLM_Studies.git {REPO_DIR}\n",
    "os.chdir(REPO_DIR)\n",
    "print('Working dir:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from data_sources.aguvis.stage1 import load_stage1, STAGE1_CONFIGS\n",
    "from data_sources.aguvis.stage2 import load_stage2, STAGE2_CONFIGS\n",
    "from data_sources.screenspot import load_screenspot\n",
    "\n",
    "print('Stage-1 configs:', STAGE1_CONFIGS)\n",
    "print('Stage-2 configs:', STAGE2_CONFIGS[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-1 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import display\n",
    "\n",
    "config = 'webui350k'\n",
    "sample = next(iter(load_stage1(config, streaming=True)))\n",
    "print('Config:', config)\n",
    "print('User:', sample['user'])\n",
    "print('Assistant:', sample['assistant'])\n",
    "display(sample['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-2 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'mind2web-l2'\n",
    "sample2 = next(iter(load_stage2(config, streaming=True)))\n",
    "print('Config:', config)\n",
    "print('System:', sample2['system'][:200])\n",
    "print('User:', sample2['user'])\n",
    "print('Assistant:', sample2['assistant'][:400])\n",
    "display(sample2['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScreenSpot-v2 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenspot_sample = next(iter(load_screenspot(split='test', streaming=True)))\n",
    "print('Instruction:', screenspot_sample.instruction)\n",
    "print('Source:', screenspot_sample.source)\n",
    "display(screenspot_sample.draw_bbox())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SmolVLM2-2.2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: authenticate if the model is gated\n",
    "from huggingface_hub import login\n",
    "# login(token='hf_...')  # uncomment and paste your token if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n\n",
    "MODEL_NAME = 'HuggingFaceTB/SmolVLM2-2.2B-Instruct'\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_NAME, trust_remote_code=True, device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "print('Model loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Trial (Stage-1 instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image', 'image': sample['image']},\n",
    "            {'type': 'text', 'text': sample['user']}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(**inputs, max_new_tokens=64)\n",
    "output = processor.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- Extend with Stage-2 multi-step prompts.\n",
    "- Log outputs to Drive for qualitative comparison.\n",
    "- Integrate bounding-box tool once available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}